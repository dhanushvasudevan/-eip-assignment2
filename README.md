# -eip-assignment2

Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 124s 2ms/step - loss: 0.5630 - acc: 0.8405 - val_loss: 0.1049 - val_acc: 0.9817
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
60000/60000 [==============================] - 120s 2ms/step - loss: 0.2589 - acc: 0.9211 - val_loss: 0.0672 - val_acc: 0.9861
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
60000/60000 [==============================] - 120s 2ms/step - loss: 0.2036 - acc: 0.9376 - val_loss: 0.0504 - val_acc: 0.9893
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
60000/60000 [==============================] - 120s 2ms/step - loss: 0.1770 - acc: 0.9442 - val_loss: 0.0392 - val_acc: 0.9904
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
60000/60000 [==============================] - 122s 2ms/step - loss: 0.1586 - acc: 0.9487 - val_loss: 0.0372 - val_acc: 0.9916
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
60000/60000 [==============================] - 121s 2ms/step - loss: 0.1423 - acc: 0.9506 - val_loss: 0.0279 - val_acc: 0.9932
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
60000/60000 [==============================] - 122s 2ms/step - loss: 0.1368 - acc: 0.9519 - val_loss: 0.0275 - val_acc: 0.9923
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
60000/60000 [==============================] - 121s 2ms/step - loss: 0.1275 - acc: 0.9525 - val_loss: 0.0267 - val_acc: 0.9923
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
60000/60000 [==============================] - 122s 2ms/step - loss: 0.1211 - acc: 0.9530 - val_loss: 0.0251 - val_acc: 0.9932
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
60000/60000 [==============================] - 122s 2ms/step - loss: 0.1157 - acc: 0.9545 - val_loss: 0.0261 - val_acc: 0.9918
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
60000/60000 [==============================] - 122s 2ms/step - loss: 0.1104 - acc: 0.9547 - val_loss: 0.0226 - val_acc: 0.9940
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
60000/60000 [==============================] - 122s 2ms/step - loss: 0.1091 - acc: 0.9563 - val_loss: 0.0244 - val_acc: 0.9932
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
60000/60000 [==============================] - 122s 2ms/step - loss: 0.1064 - acc: 0.9566 - val_loss: 0.0250 - val_acc: 0.9930
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
60000/60000 [==============================] - 121s 2ms/step - loss: 0.1030 - acc: 0.9568 - val_loss: 0.0207 - val_acc: 0.9939
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
60000/60000 [==============================] - 121s 2ms/step - loss: 0.1033 - acc: 0.9551 - val_loss: 0.0238 - val_acc: 0.9927
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
60000/60000 [==============================] - 122s 2ms/step - loss: 0.0985 - acc: 0.9574 - val_loss: 0.0230 - val_acc: 0.9939
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
60000/60000 [==============================] - 123s 2ms/step - loss: 0.0976 - acc: 0.9571 - val_loss: 0.0215 - val_acc: 0.9941
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
60000/60000 [==============================] - 123s 2ms/step - loss: 0.0978 - acc: 0.9574 - val_loss: 0.0215 - val_acc: 0.9951
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
60000/60000 [==============================] - 124s 2ms/step - loss: 0.0965 - acc: 0.9564 - val_loss: 0.0197 - val_acc: 0.9937
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 123s 2ms/step - loss: 0.0944 - acc: 0.9575 - val_loss: 0.0217 - val_acc: 0.9943
[0.021746620394289495, 0.9943]
stategies made in the assignment 
trying to make an compact size of layers 
tryed fit appropriate number of channels and max pooling every layer once 
and kept the batch normalisation and dropout features remain same as the 8th code 
and team mate vihar
